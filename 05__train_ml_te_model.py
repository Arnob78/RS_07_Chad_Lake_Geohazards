import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import joblib
import json
from datetime import datetime

def create_output_directory(script_path):
    """Creates a directory with the same name as the script to save outputs."""
    script_name = os.path.basename(script_path)
    folder_name = os.path.splitext(script_name)[0]
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
    return folder_name

def build_fnn_model(input_shape):
    """
    Builds the Feed-forward Neural Network (FNN) model as described in the base paper.
    The architecture is kept simple but effective for this regression task.
    """
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_shape,)),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(32, activation='relu'),
        # The final layer is a single neuron for the regression output (the TTE value)
        Dense(1)
    ])
    
    # Compile the model
    # We use Mean Squared Error as the loss function, which is standard for regression.
    # The Adam optimizer is a good default choice.
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
    
    return model

def save_model_architecture_csv(model, output_path, input_shape):
    """Saves the model architecture to a CSV file - PROPERLY FIXED VERSION"""
    architecture_data = []
    
    # Manually track the shapes since direct access is problematic
    current_shape = input_shape
    
    for i, layer in enumerate(model.layers):
        layer_type = layer.__class__.__name__
        
        # Calculate input and output shapes manually
        input_shape_str = f"(None, {current_shape})"
        
        if layer_type == 'Dense':
            units = layer.units
            output_shape_str = f"(None, {units})"
            current_shape = units  # Update for next layer
        elif layer_type == 'Dropout':
            output_shape_str = f"(None, {current_shape})"  # Dropout doesn't change shape
        else:
            output_shape_str = "N/A"
        
        # Get activation function
        if hasattr(layer, 'activation'):
            activation = layer.activation.__name__
        else:
            activation = 'N/A'
        
        # Get parameter count
        param_count = layer.count_params()
        
        architecture_data.append({
            'Layer (Type)': layer_type,
            'Input Shape': input_shape_str,
            'Output Shape': output_shape_str,
            'Activation': activation,
            'Param #': param_count
        })
    
    # Create DataFrame and save to CSV
    df = pd.DataFrame(architecture_data)
    df.to_csv(output_path, index=False)
    print(f"Model architecture saved to: '{output_path}'")

if __name__ == '__main__':
    # --- Setup ---
    # Create the output directory based on the script name
    output_folder = create_output_directory(__file__)
    print(f"Output folder '{output_folder}' created.")

    # --- Load Data ---
    # Define the input data file from the previous step
    # These parameters must match the file generated by 02_generate_synthetic_data.py
    M = 2
    L = 24
    input_csv = os.path.join('02__generate_synthetic_data', f'synthetic_training_data_M{M}_L{L}.csv')
    
    print(f"Loading training data from '{input_csv}'...")
    df = pd.read_csv(input_csv)

    # --- Prepare Data for Training ---
    # Separate features (X) from the target variable (y)
    X = df.drop(columns=['tte', 'sequence_length', 'M'])
    y = df['tte']

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale the features. This is crucial for neural network performance.
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    input_shape = X_train_scaled.shape[1]
    print(f"Data prepared: {len(X_train)} training samples, {len(X_test)} testing samples.")
    print(f"Input shape: {input_shape}")

    # --- Build and Train the Model ---
    model = build_fnn_model(input_shape=input_shape)
    model.summary()

    # --- NEW: Save model architecture to CSV ---
    architecture_csv_path = os.path.join(output_folder, 'table_3_model_architecture.csv')
    save_model_architecture_csv(model, architecture_csv_path, input_shape)

    # Use EarlyStopping to prevent overfitting
    # It will stop training if the validation loss doesn't improve for a number of epochs.
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    print("\nTraining the model...")
    history = model.fit(
        X_train_scaled,
        y_train,
        epochs=100, # Set a high number of epochs; EarlyStopping will find the optimal number
        batch_size=32,
        validation_split=0.2, # Use part of the training data for validation
        callbacks=[early_stopping],
        verbose=1
    )

    # --- Evaluate the Model ---
    print("\nEvaluating the model on the test set...")
    test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)
    print(f"Test Loss (MSE): {test_loss:.6f}")
    print(f"Test Mean Absolute Error (MAE): {test_mae:.6f}")

    # --- Save the Trained Model and Scaler ---
    # The scaler must be saved to process our real data later
    model_save_path = os.path.join(output_folder, 'ml_te_model.h5')
    scaler_save_path = os.path.join(output_folder, 'scaler.joblib')
    
    model.save(model_save_path)
    joblib.dump(scaler, scaler_save_path)

    print(f"\nTrained model saved to: '{model_save_path}'")
    print(f"Scaler saved to: '{scaler_save_path}'")

    # --- NEW: Save Evaluation Metrics to JSON File ---
    metrics = {
        'test_mse': float(test_loss),
        'test_mae': float(test_mae),
        'training_samples': len(X_train),
        'test_samples': len(X_test),
        'timestamp': datetime.now().isoformat(),
        'model_architecture': {
            'hidden_layers': [128, 64, 32],
            'dropout_rate': 0.2,
            'optimizer': 'adam',
            'loss_function': 'mean_squared_error'
        }
    }

    metrics_path = os.path.join(output_folder, 'model_metrics.json')
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)

    print(f"Model metrics saved to: '{metrics_path}'")
    
    # --- NEW: Print clear final metrics for the paper ---
    print(f"\n" + "="*60)
    print("FINAL METRICS FOR PAPER:")
    print("="*60)
    print(f"Test Loss (MSE): {test_loss:.6f}")
    print(f"Test Mean Absolute Error (MAE): {test_mae:.6f}")
    print("="*60)
    print("Copy these values into your manuscript:")
    print(f"MAE: {test_mae:.6f}")
    print(f"MSE: {test_loss:.6f}")
    print("="*60)